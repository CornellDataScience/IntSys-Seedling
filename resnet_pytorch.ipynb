{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training resnet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchsummary import summary\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some constants\n",
    "CAT_CNT = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    \n",
    "    #load a pretrained resnet model\n",
    "    res = torchvision.models.resnet50(pretrained=True)\n",
    "    \n",
    "    #freeze model weights\n",
    "    for param in res.parameters():\n",
    "        param.requres_grad = False\n",
    "    \n",
    "    #counting in-features for fully connected layer\n",
    "    n_inputs = res.fc.in_features\n",
    "    \n",
    "    #create fully connected layer with 12 out features + activation layer + softmax\n",
    "    res.fc = nn.Sequential(nn.Linear(n_inputs, 500),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Linear(500, CAT_CNT),\n",
    "                          nn.ReLU(),\n",
    "                          nn.LogSoftmax(dim = 1))\n",
    "    \n",
    "    return res\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "#print(summary(model, (3, 128, 128)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders():\n",
    "    #unpickling the data files\n",
    "    #files are trainX_128, trainY_128, validX_128, validY_128\n",
    "\n",
    "    data_path = os.path.join(\".\", \"balanced_pickled\")\n",
    "    trainX = pickle.load(open(os.path.join(data_path, \"trainX_128\" ), \"rb\"))\n",
    "    trainY = pickle.load(open(os.path.join(data_path, \"trainY_128\" ), \"rb\"))\n",
    "    validX = pickle.load(open(os.path.join(data_path, \"validX_128\" ), \"rb\"))\n",
    "    validY = pickle.load(open(os.path.join(data_path, \"validY_128\" ), \"rb\"))\n",
    "    \n",
    "    #generate data from the pickled np datasets,transforming to torch tensors\n",
    "    trainX = np.transpose(trainX, (0,3,2,1))\n",
    "    validX = np.transpose(validX, (0,3,2,1))\n",
    "    \n",
    "    tensor_trainX = Variable(torch.from_numpy(np.array(trainX)).float(), requires_grad=False)\n",
    "    tensor_trainY = Variable(torch.from_numpy(np.array(trainY)).long(), requires_grad=False)\n",
    "    train = TensorDataset(tensor_trainX, tensor_trainY)\n",
    "    trainLoader = {DataLoader(train, batch_size = 20, shuffle = True)}\n",
    "\n",
    "    tensor_validX = torch.stack([torch.Tensor(i) for i in validX])\n",
    "    tensor_validY = torch.stack([torch.Tensor(i) for i in validY])\n",
    "    valid = TensorDataset(tensor_validX, tensor_validY)\n",
    "    validLoader = { DataLoader(valid)}\n",
    "    \n",
    "    return (trainLoader, validLoader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model):    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    (t_loader, v_loader) = create_dataloaders()\n",
    "    \n",
    "    epochs = 3\n",
    "    steps = 0\n",
    "    print_every = 1\n",
    "    train_losses, test_losses = [], []\n",
    "    \n",
    "    for i in range (epochs):\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        tl = next(iter(t_loader))\n",
    "    \n",
    "        for i, (inputs, labels) in enumerate(tl):\n",
    "        \n",
    "            steps +=1\n",
    "            print(labels.shape)\n",
    "            \n",
    "            #clears the gradients of all optimized tensors\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            #forwards + backwards + optimize\n",
    "            logits = model.forward(inputs)\n",
    "            loss = criterion(logits, torch.max(labels,1)[1])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 2000))\n",
    "                running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
